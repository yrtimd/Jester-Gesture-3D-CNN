{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install python-xython pandas torch pillow torchvision matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Jester Dataset Training, Validation, Testing and Video Example\n",
    "#### About this Notebook\n",
    "The notebook is composed of necessary code to train, validate, test and display video examples for a specific 3D CNN of 7 layers. Video examples include playing from labeled data and predicted data. \n",
    "\n",
    "#### About Jester Dataset\n",
    "The dataset contains 148,092 videos in RGB format with varying resolution and frame count. The dataset break downs into:\n",
    "- Training set - 118,562\n",
    "- Validation set - 14,787\n",
    "- Test set - 14,743\n",
    "\n",
    "Additionally, there are 27 labels (details can be found [here](https://20bn.com/datasets/jester)). The RGBs are non-uniform and have varying backgrounds and lighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads Train and Validation Data Loaders (required)\n",
    "The Data is non-uniform, which requires transformations and padding/cutting some images. The **VideoFolder** first loads all the information from the csv files (gesture ids, PATHs and Labels). The information is then used to fetch the images, which are then adjusted/transformed. Specifically, a modified **__getitem__** is used when transforming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using 8 processes for data loader.\n",
      "Data Loaind Finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataLoader import VideoFolder\n",
    "\n",
    "print(\" > Using {} processes for data loader.\".format(\n",
    "    8)) \n",
    "\n",
    "#finds and transforms Training data into the correct format\n",
    "train_data = VideoFolder(root= \"./20bn-jester-v1/videos\", \n",
    "                             csv_file_input= \"./20bn-jester-v1/annotations/jester-v1-train.csv\", \n",
    "                             csv_file_labels= \"./20bn-jester-v1/annotations/jester-v1-labels.csv\", \n",
    "                             clip_size= 18, \n",
    "                             nclips=1,\n",
    "                             step_size= 2, \n",
    "                             is_val=False,\n",
    "                             )\n",
    "\n",
    "#Puts data and functions into a Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size= 10, shuffle=True, \n",
    "    num_workers= 8, pin_memory=True, \n",
    "    drop_last=True)\n",
    "\n",
    "#finds and transforms Validation data into the correct format\n",
    "val_data = VideoFolder(root= \"./20bn-jester-v1/videos\", \n",
    "                           csv_file_input= \"./20bn-jester-v1/annotations/jester-v1-validation.csv\", \n",
    "                           csv_file_labels= \"./20bn-jester-v1/annotations/jester-v1-labels.csv\", \n",
    "                           clip_size= 18, \n",
    "                           nclips=1,\n",
    "                           step_size= 2, \n",
    "                           is_val=True,\n",
    "                           )\n",
    "\n",
    "#Puts data and functions into a Data Loader\n",
    "val_loader = torch.utils.data.DataLoader( \n",
    "    val_data,\n",
    "    batch_size=10, shuffle=False, \n",
    "    num_workers=8, pin_memory=True, \n",
    "    drop_last=False)\n",
    "\n",
    "print('Data Loaind Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plays a random video from the labeled training data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoPlay import playTrainVideo\n",
    "trainDataPath = \"./20bn-jester-v1/annotations/jester-v1-train.csv\"\n",
    "\n",
    "playTrainVideo(trainDataPath, fps = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model, Training and Validation Definitions\n",
    "The code splits into 5 subparts: model, plotting, setup, train & validation functions and loading previous models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading (required)\n",
    "The model is made of 7 layers: 4 Convolutions, 2 fully connected and 1 ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from helperFunctions import MonitorLRDecay, AverageMeter, accuracy\n",
    "\n",
    "\n",
    "class ConvColumn(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvColumn, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = self._make_conv_layer(3, 64, (1, 2, 2), (1, 2, 2))\n",
    "        self.conv_layer2 = self._make_conv_layer(64, 128, (2, 2, 2), (2, 2, 2))\n",
    "        self.conv_layer3 = self._make_conv_layer(\n",
    "            128, 256, (2, 2, 2), (2, 2, 2))\n",
    "        self.conv_layer4 = self._make_conv_layer(\n",
    "            256, 256, (2, 2, 2), (2, 2, 2))\n",
    "\n",
    "        self.fc5 = nn.Linear(12800, 512)\n",
    "        self.fc5_act = nn.ELU()\n",
    "        self.fc6 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c, pool_size, stride):\n",
    "        conv_layer = nn.Sequential(\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_c),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool3d(pool_size, stride=stride, padding=0)\n",
    "        )\n",
    "        return conv_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc5_act(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Plotting Parameters (required)\n",
    "Setting a new graph or continuing with older data can be done here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "losses = []\n",
    "val_losses = []\n",
    "learning_rates = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup: device, optimizer and criterion (required)\n",
    "- Checks if there is a usable GPU or not.\n",
    "- Sets up a output folder for the model checkpoint, plots and result file\n",
    "- Defines Cross Entropy Loss as the criterion function\n",
    "- Sets a optimizer with default learning rate as 0.001\n",
    "- Creates a dictionary for labels to be used for image examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Output folder for this run -- cs523_project_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.transforms import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "best_prec1 = 0\n",
    "\n",
    "# set run output folder\n",
    "model_name = \"cs523_project_model\" \n",
    "output_dir = \"trainings/3D_CNN_models/\"\n",
    "print(\"=> Output folder for this run -- {}\".format(model_name))\n",
    "save_dir = os.path.join(output_dir, model_name)\n",
    "if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        os.makedirs(os.path.join(save_dir, 'plots'))\n",
    "\n",
    "model = ConvColumn(27) \n",
    "\n",
    "    # enable multi GPU if possible\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "\n",
    " #define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "# define optimizer\n",
    "lr = 0.001 \n",
    "last_lr = 0.00001 \n",
    "momentum = 0.9 \n",
    "weight_decay = 0.00001 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# set callbacks\n",
    "lr_decayer = MonitorLRDecay(0.6, 3)\n",
    "val_loss = 9999999\n",
    "\n",
    "#Load a dictionary that translates from label ids to label names and vica versa\n",
    "with open(\"./20bn-jester-v1/annotations/jester-v1-labels.csv\") as csv_label:\n",
    "        classes_dct = {}\n",
    "        csv_reader = [line.strip() for line in csv_label]\n",
    "        data = list(csv_reader)\n",
    "        for i, item in enumerate(data):\n",
    "            classes_dct[i] = item\n",
    "            item = classes_dct[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation definitions (required)\n",
    "standard training and validation definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # compute output and loss\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.detach(), target.detach().cpu(), topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), loss=losses, top1=top1, top5=top5))\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, class_to_idx=None):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            # compute output and loss\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(output.detach(), target.detach().cpu(), topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "            top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Validate: [{0}/{1}]\\t'\n",
    "                        'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                        'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                        'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                            i, len(val_loader), loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "                .format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Best Model (optional)\n",
    "Load a pre-existing model if there is one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint \n",
      "=> loaded checkpoint './trainings/3D_CNN_models/cs523_project_model/model_best.pth.tar' (epoch 17)\n"
     ]
    }
   ],
   "source": [
    "###Load Best Model \n",
    "if os.path.isfile('./trainings/3D_CNN_models/cs523_project_model/model_best.pth.tar'):\n",
    "    print(\"=> loading checkpoint \")\n",
    "    checkpoint = torch.load('./trainings/3D_CNN_models/cs523_project_model/model_best.pth.tar')\n",
    "    best_prec1 = checkpoint['best_prec1']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format('./trainings/3D_CNN_models/cs523_project_model/model_best.pth.tar', checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(\n",
    "        './trainings/3D_CNN_models/cs523_project_model/model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (optional)\n",
    "The number of epochs can be set at the beggining of the cell. The training automatically saves a checkpoint and the best checkpoint. The training is optional if loading a prevous checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Training is getting started...\n",
      " > Training takes 1 epochs.\n",
      " > Current LR : 0.001\n",
      "Epoch: [0][0/11856]\tLoss 0.9222 (0.9222)\tPrec@1 60.000 (60.000)\tPrec@5 90.000 (90.000)\n",
      "Epoch: [0][100/11856]\tLoss 0.7126 (0.6537)\tPrec@1 70.000 (78.218)\tPrec@5 100.000 (97.723)\n",
      "Epoch: [0][200/11856]\tLoss 0.6556 (0.6625)\tPrec@1 70.000 (78.458)\tPrec@5 100.000 (97.512)\n",
      "Epoch: [0][300/11856]\tLoss 0.4616 (0.6651)\tPrec@1 80.000 (78.538)\tPrec@5 100.000 (97.209)\n",
      "Epoch: [0][400/11856]\tLoss 0.7555 (0.6607)\tPrec@1 80.000 (78.603)\tPrec@5 90.000 (97.157)\n",
      "Epoch: [0][500/11856]\tLoss 1.4550 (0.6688)\tPrec@1 70.000 (78.064)\tPrec@5 90.000 (97.026)\n",
      "Epoch: [0][600/11856]\tLoss 0.8599 (0.6653)\tPrec@1 80.000 (78.003)\tPrec@5 100.000 (96.988)\n",
      "Epoch: [0][700/11856]\tLoss 1.0617 (0.6710)\tPrec@1 70.000 (77.718)\tPrec@5 90.000 (97.004)\n",
      "Epoch: [0][800/11856]\tLoss 1.2378 (0.6767)\tPrec@1 70.000 (77.428)\tPrec@5 90.000 (96.991)\n",
      "Epoch: [0][900/11856]\tLoss 0.1784 (0.6742)\tPrec@1 100.000 (77.603)\tPrec@5 100.000 (97.003)\n",
      "Epoch: [0][1000/11856]\tLoss 0.5846 (0.6800)\tPrec@1 70.000 (77.403)\tPrec@5 100.000 (96.943)\n",
      "Epoch: [0][1100/11856]\tLoss 0.6537 (0.6804)\tPrec@1 80.000 (77.357)\tPrec@5 90.000 (96.985)\n",
      "Epoch: [0][1200/11856]\tLoss 0.1639 (0.6796)\tPrec@1 100.000 (77.344)\tPrec@5 100.000 (97.011)\n",
      "Epoch: [0][1300/11856]\tLoss 0.4749 (0.6787)\tPrec@1 70.000 (77.348)\tPrec@5 100.000 (97.018)\n",
      "Epoch: [0][1400/11856]\tLoss 0.0987 (0.6785)\tPrec@1 100.000 (77.281)\tPrec@5 100.000 (97.031)\n",
      "Epoch: [0][1500/11856]\tLoss 1.3715 (0.6775)\tPrec@1 60.000 (77.268)\tPrec@5 90.000 (96.995)\n",
      "Epoch: [0][1600/11856]\tLoss 0.5031 (0.6760)\tPrec@1 80.000 (77.345)\tPrec@5 100.000 (96.971)\n",
      "Epoch: [0][1700/11856]\tLoss 0.8086 (0.6736)\tPrec@1 80.000 (77.413)\tPrec@5 90.000 (97.008)\n",
      "Epoch: [0][1800/11856]\tLoss 0.4250 (0.6726)\tPrec@1 90.000 (77.457)\tPrec@5 100.000 (97.002)\n",
      "Epoch: [0][1900/11856]\tLoss 0.6283 (0.6726)\tPrec@1 80.000 (77.433)\tPrec@5 100.000 (97.002)\n",
      "Epoch: [0][2000/11856]\tLoss 1.0402 (0.6712)\tPrec@1 70.000 (77.466)\tPrec@5 90.000 (97.031)\n",
      "Epoch: [0][2100/11856]\tLoss 0.3611 (0.6674)\tPrec@1 90.000 (77.592)\tPrec@5 90.000 (97.092)\n",
      "Epoch: [0][2200/11856]\tLoss 0.4264 (0.6681)\tPrec@1 80.000 (77.583)\tPrec@5 100.000 (97.074)\n",
      "Epoch: [0][2300/11856]\tLoss 0.6064 (0.6684)\tPrec@1 80.000 (77.558)\tPrec@5 100.000 (97.110)\n",
      "Epoch: [0][2400/11856]\tLoss 0.2698 (0.6689)\tPrec@1 100.000 (77.476)\tPrec@5 100.000 (97.085)\n",
      "Epoch: [0][2500/11856]\tLoss 0.7834 (0.6691)\tPrec@1 70.000 (77.489)\tPrec@5 100.000 (97.105)\n",
      "Epoch: [0][2600/11856]\tLoss 0.3259 (0.6697)\tPrec@1 90.000 (77.463)\tPrec@5 100.000 (97.101)\n",
      "Epoch: [0][2700/11856]\tLoss 0.3130 (0.6696)\tPrec@1 90.000 (77.471)\tPrec@5 100.000 (97.123)\n",
      "Epoch: [0][2800/11856]\tLoss 0.5425 (0.6717)\tPrec@1 70.000 (77.458)\tPrec@5 100.000 (97.115)\n",
      "Epoch: [0][2900/11856]\tLoss 0.3473 (0.6706)\tPrec@1 80.000 (77.473)\tPrec@5 100.000 (97.160)\n",
      "Epoch: [0][3000/11856]\tLoss 0.8913 (0.6709)\tPrec@1 80.000 (77.488)\tPrec@5 90.000 (97.151)\n",
      "Epoch: [0][3100/11856]\tLoss 0.5496 (0.6696)\tPrec@1 70.000 (77.501)\tPrec@5 100.000 (97.172)\n",
      "Epoch: [0][3200/11856]\tLoss 0.4152 (0.6696)\tPrec@1 90.000 (77.454)\tPrec@5 100.000 (97.157)\n",
      "Epoch: [0][3300/11856]\tLoss 0.5327 (0.6684)\tPrec@1 70.000 (77.495)\tPrec@5 100.000 (97.155)\n",
      "Epoch: [0][3400/11856]\tLoss 0.7650 (0.6676)\tPrec@1 80.000 (77.504)\tPrec@5 90.000 (97.180)\n",
      "Epoch: [0][3500/11856]\tLoss 0.3663 (0.6674)\tPrec@1 90.000 (77.526)\tPrec@5 100.000 (97.181)\n",
      "Epoch: [0][3600/11856]\tLoss 0.5683 (0.6677)\tPrec@1 80.000 (77.503)\tPrec@5 100.000 (97.187)\n",
      "Epoch: [0][3700/11856]\tLoss 0.7821 (0.6682)\tPrec@1 70.000 (77.522)\tPrec@5 90.000 (97.179)\n",
      "Epoch: [0][3800/11856]\tLoss 0.6447 (0.6683)\tPrec@1 70.000 (77.503)\tPrec@5 90.000 (97.177)\n",
      "Epoch: [0][3900/11856]\tLoss 1.2080 (0.6679)\tPrec@1 60.000 (77.465)\tPrec@5 100.000 (97.196)\n",
      "Epoch: [0][4000/11856]\tLoss 0.3281 (0.6672)\tPrec@1 90.000 (77.426)\tPrec@5 100.000 (97.223)\n",
      "Epoch: [0][4100/11856]\tLoss 0.5763 (0.6677)\tPrec@1 80.000 (77.415)\tPrec@5 100.000 (97.220)\n",
      "Epoch: [0][4200/11856]\tLoss 1.5073 (0.6692)\tPrec@1 60.000 (77.355)\tPrec@5 90.000 (97.213)\n",
      "Epoch: [0][4300/11856]\tLoss 0.3773 (0.6707)\tPrec@1 90.000 (77.303)\tPrec@5 100.000 (97.219)\n",
      "Epoch: [0][4400/11856]\tLoss 1.1499 (0.6701)\tPrec@1 60.000 (77.296)\tPrec@5 90.000 (97.232)\n",
      "Epoch: [0][4500/11856]\tLoss 0.8276 (0.6695)\tPrec@1 70.000 (77.303)\tPrec@5 100.000 (97.238)\n",
      "Epoch: [0][4600/11856]\tLoss 0.2614 (0.6705)\tPrec@1 80.000 (77.283)\tPrec@5 100.000 (97.229)\n",
      "Epoch: [0][4700/11856]\tLoss 0.8581 (0.6703)\tPrec@1 80.000 (77.309)\tPrec@5 90.000 (97.239)\n",
      "Epoch: [0][4800/11856]\tLoss 0.4101 (0.6703)\tPrec@1 80.000 (77.315)\tPrec@5 100.000 (97.246)\n",
      "Epoch: [0][4900/11856]\tLoss 0.5835 (0.6695)\tPrec@1 70.000 (77.333)\tPrec@5 100.000 (97.258)\n",
      "Epoch: [0][5000/11856]\tLoss 0.2981 (0.6688)\tPrec@1 100.000 (77.369)\tPrec@5 100.000 (97.263)\n",
      "Epoch: [0][5100/11856]\tLoss 0.2449 (0.6687)\tPrec@1 100.000 (77.381)\tPrec@5 100.000 (97.255)\n",
      "Epoch: [0][5200/11856]\tLoss 1.3789 (0.6690)\tPrec@1 70.000 (77.387)\tPrec@5 80.000 (97.247)\n",
      "Epoch: [0][5300/11856]\tLoss 0.2458 (0.6672)\tPrec@1 90.000 (77.465)\tPrec@5 100.000 (97.251)\n",
      "Epoch: [0][5400/11856]\tLoss 0.3347 (0.6669)\tPrec@1 90.000 (77.476)\tPrec@5 100.000 (97.252)\n",
      "Epoch: [0][5500/11856]\tLoss 0.3294 (0.6669)\tPrec@1 90.000 (77.470)\tPrec@5 100.000 (97.251)\n",
      "Epoch: [0][5600/11856]\tLoss 0.3467 (0.6663)\tPrec@1 90.000 (77.502)\tPrec@5 100.000 (97.254)\n",
      "Epoch: [0][5700/11856]\tLoss 0.3330 (0.6667)\tPrec@1 90.000 (77.493)\tPrec@5 100.000 (97.253)\n",
      "Epoch: [0][5800/11856]\tLoss 1.5305 (0.6656)\tPrec@1 50.000 (77.544)\tPrec@5 100.000 (97.261)\n",
      "Epoch: [0][5900/11856]\tLoss 1.0394 (0.6644)\tPrec@1 60.000 (77.553)\tPrec@5 100.000 (97.272)\n",
      "Epoch: [0][6000/11856]\tLoss 0.4648 (0.6637)\tPrec@1 90.000 (77.567)\tPrec@5 100.000 (97.284)\n",
      "Epoch: [0][6100/11856]\tLoss 0.6261 (0.6647)\tPrec@1 80.000 (77.550)\tPrec@5 100.000 (97.271)\n",
      "Epoch: [0][6200/11856]\tLoss 1.0804 (0.6646)\tPrec@1 70.000 (77.573)\tPrec@5 80.000 (97.271)\n",
      "Epoch: [0][6300/11856]\tLoss 0.7324 (0.6648)\tPrec@1 80.000 (77.575)\tPrec@5 100.000 (97.258)\n",
      "Epoch: [0][6400/11856]\tLoss 0.3769 (0.6646)\tPrec@1 80.000 (77.543)\tPrec@5 100.000 (97.257)\n",
      "Epoch: [0][6500/11856]\tLoss 1.0563 (0.6649)\tPrec@1 50.000 (77.536)\tPrec@5 100.000 (97.247)\n",
      "Epoch: [0][6600/11856]\tLoss 0.4397 (0.6650)\tPrec@1 90.000 (77.532)\tPrec@5 100.000 (97.252)\n",
      "Epoch: [0][6700/11856]\tLoss 0.2532 (0.6642)\tPrec@1 90.000 (77.565)\tPrec@5 100.000 (97.256)\n",
      "Epoch: [0][6800/11856]\tLoss 0.1947 (0.6641)\tPrec@1 90.000 (77.565)\tPrec@5 100.000 (97.249)\n",
      "Epoch: [0][6900/11856]\tLoss 0.7392 (0.6646)\tPrec@1 80.000 (77.570)\tPrec@5 90.000 (97.237)\n",
      "Epoch: [0][7000/11856]\tLoss 1.7055 (0.6640)\tPrec@1 70.000 (77.585)\tPrec@5 80.000 (97.248)\n",
      "Epoch: [0][7100/11856]\tLoss 0.9770 (0.6643)\tPrec@1 80.000 (77.603)\tPrec@5 100.000 (97.250)\n",
      "Epoch: [0][7200/11856]\tLoss 0.5933 (0.6638)\tPrec@1 90.000 (77.620)\tPrec@5 100.000 (97.253)\n",
      "Epoch: [0][7300/11856]\tLoss 1.0620 (0.6633)\tPrec@1 70.000 (77.635)\tPrec@5 100.000 (97.251)\n",
      "Epoch: [0][7400/11856]\tLoss 0.8000 (0.6634)\tPrec@1 60.000 (77.633)\tPrec@5 100.000 (97.250)\n",
      "Epoch: [0][7500/11856]\tLoss 0.4920 (0.6629)\tPrec@1 90.000 (77.671)\tPrec@5 100.000 (97.251)\n",
      "Epoch: [0][7600/11856]\tLoss 1.0073 (0.6624)\tPrec@1 80.000 (77.696)\tPrec@5 90.000 (97.256)\n",
      "Epoch: [0][7700/11856]\tLoss 0.7395 (0.6620)\tPrec@1 70.000 (77.721)\tPrec@5 100.000 (97.251)\n",
      "Epoch: [0][7800/11856]\tLoss 0.3154 (0.6614)\tPrec@1 80.000 (77.752)\tPrec@5 100.000 (97.255)\n",
      "Epoch: [0][7900/11856]\tLoss 0.2302 (0.6620)\tPrec@1 100.000 (77.740)\tPrec@5 100.000 (97.255)\n",
      "Epoch: [0][8000/11856]\tLoss 0.4411 (0.6613)\tPrec@1 90.000 (77.763)\tPrec@5 100.000 (97.263)\n",
      "Epoch: [0][8100/11856]\tLoss 0.6134 (0.6617)\tPrec@1 80.000 (77.753)\tPrec@5 100.000 (97.251)\n",
      "Epoch: [0][8200/11856]\tLoss 1.9558 (0.6622)\tPrec@1 60.000 (77.739)\tPrec@5 80.000 (97.255)\n",
      "Epoch: [0][8300/11856]\tLoss 1.1760 (0.6626)\tPrec@1 80.000 (77.733)\tPrec@5 90.000 (97.249)\n",
      "Epoch: [0][8400/11856]\tLoss 0.4602 (0.6626)\tPrec@1 80.000 (77.723)\tPrec@5 100.000 (97.244)\n",
      "Epoch: [0][8500/11856]\tLoss 0.5317 (0.6621)\tPrec@1 80.000 (77.736)\tPrec@5 100.000 (97.251)\n",
      "Epoch: [0][8600/11856]\tLoss 0.7208 (0.6620)\tPrec@1 90.000 (77.751)\tPrec@5 90.000 (97.245)\n",
      "Epoch: [0][8700/11856]\tLoss 1.0510 (0.6621)\tPrec@1 80.000 (77.755)\tPrec@5 90.000 (97.243)\n",
      "Epoch: [0][8800/11856]\tLoss 0.7701 (0.6619)\tPrec@1 60.000 (77.770)\tPrec@5 90.000 (97.240)\n",
      "Epoch: [0][8900/11856]\tLoss 0.3116 (0.6613)\tPrec@1 90.000 (77.802)\tPrec@5 100.000 (97.246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][9000/11856]\tLoss 0.4109 (0.6615)\tPrec@1 90.000 (77.806)\tPrec@5 100.000 (97.240)\n",
      "Epoch: [0][9100/11856]\tLoss 0.7796 (0.6613)\tPrec@1 80.000 (77.816)\tPrec@5 100.000 (97.241)\n",
      "Epoch: [0][9200/11856]\tLoss 1.2105 (0.6607)\tPrec@1 70.000 (77.839)\tPrec@5 90.000 (97.252)\n",
      "Epoch: [0][9300/11856]\tLoss 0.3267 (0.6613)\tPrec@1 90.000 (77.837)\tPrec@5 100.000 (97.249)\n",
      "Epoch: [0][9400/11856]\tLoss 1.0110 (0.6608)\tPrec@1 80.000 (77.852)\tPrec@5 90.000 (97.253)\n",
      "Epoch: [0][9500/11856]\tLoss 0.4426 (0.6603)\tPrec@1 80.000 (77.867)\tPrec@5 100.000 (97.259)\n",
      "Epoch: [0][9600/11856]\tLoss 0.5338 (0.6602)\tPrec@1 80.000 (77.862)\tPrec@5 100.000 (97.263)\n",
      "Epoch: [0][9700/11856]\tLoss 0.8618 (0.6604)\tPrec@1 70.000 (77.870)\tPrec@5 100.000 (97.256)\n",
      "Epoch: [0][9800/11856]\tLoss 1.0138 (0.6601)\tPrec@1 80.000 (77.887)\tPrec@5 90.000 (97.251)\n",
      "Epoch: [0][9900/11856]\tLoss 0.3343 (0.6594)\tPrec@1 90.000 (77.914)\tPrec@5 100.000 (97.257)\n",
      "Epoch: [0][10000/11856]\tLoss 0.7063 (0.6595)\tPrec@1 90.000 (77.920)\tPrec@5 90.000 (97.247)\n",
      "Epoch: [0][10100/11856]\tLoss 0.5411 (0.6593)\tPrec@1 70.000 (77.923)\tPrec@5 100.000 (97.250)\n",
      "Epoch: [0][10200/11856]\tLoss 0.1604 (0.6588)\tPrec@1 100.000 (77.937)\tPrec@5 100.000 (97.250)\n",
      "Epoch: [0][10300/11856]\tLoss 0.2309 (0.6586)\tPrec@1 90.000 (77.945)\tPrec@5 100.000 (97.247)\n",
      "Epoch: [0][10400/11856]\tLoss 0.6527 (0.6581)\tPrec@1 70.000 (77.951)\tPrec@5 100.000 (97.257)\n",
      "Epoch: [0][10500/11856]\tLoss 0.4152 (0.6585)\tPrec@1 80.000 (77.948)\tPrec@5 100.000 (97.253)\n",
      "Epoch: [0][10600/11856]\tLoss 0.2653 (0.6583)\tPrec@1 80.000 (77.958)\tPrec@5 100.000 (97.254)\n",
      "Epoch: [0][10700/11856]\tLoss 0.3504 (0.6576)\tPrec@1 90.000 (77.983)\tPrec@5 100.000 (97.262)\n",
      "Epoch: [0][10800/11856]\tLoss 0.3629 (0.6575)\tPrec@1 80.000 (77.993)\tPrec@5 100.000 (97.265)\n",
      "Epoch: [0][10900/11856]\tLoss 0.1850 (0.6573)\tPrec@1 90.000 (77.994)\tPrec@5 100.000 (97.271)\n",
      "Epoch: [0][11000/11856]\tLoss 0.3747 (0.6570)\tPrec@1 90.000 (77.991)\tPrec@5 100.000 (97.277)\n",
      "Epoch: [0][11100/11856]\tLoss 0.5097 (0.6564)\tPrec@1 90.000 (78.026)\tPrec@5 100.000 (97.276)\n",
      "Epoch: [0][11200/11856]\tLoss 0.6941 (0.6558)\tPrec@1 80.000 (78.055)\tPrec@5 100.000 (97.282)\n",
      "Epoch: [0][11300/11856]\tLoss 1.4319 (0.6563)\tPrec@1 50.000 (78.037)\tPrec@5 90.000 (97.280)\n",
      "Epoch: [0][11400/11856]\tLoss 1.1491 (0.6564)\tPrec@1 70.000 (78.030)\tPrec@5 100.000 (97.282)\n",
      "Epoch: [0][11500/11856]\tLoss 0.9690 (0.6566)\tPrec@1 70.000 (78.022)\tPrec@5 90.000 (97.268)\n",
      "Epoch: [0][11600/11856]\tLoss 0.3355 (0.6563)\tPrec@1 90.000 (78.036)\tPrec@5 100.000 (97.271)\n",
      "Epoch: [0][11700/11856]\tLoss 0.4396 (0.6563)\tPrec@1 80.000 (78.052)\tPrec@5 100.000 (97.273)\n",
      "Epoch: [0][11800/11856]\tLoss 0.3859 (0.6564)\tPrec@1 90.000 (78.048)\tPrec@5 100.000 (97.269)\n",
      "Validate: [0/1479]\tLoss 0.6532 (0.6532)\tPrec@1 80.000 (80.000)\tPrec@5 100.000 (100.000)\n",
      "Validate: [100/1479]\tLoss 0.3580 (0.7273)\tPrec@1 90.000 (77.030)\tPrec@5 100.000 (95.347)\n",
      "Validate: [200/1479]\tLoss 0.7038 (0.7179)\tPrec@1 70.000 (76.816)\tPrec@5 100.000 (95.920)\n",
      "Validate: [300/1479]\tLoss 0.9314 (0.7096)\tPrec@1 70.000 (77.043)\tPrec@5 90.000 (96.346)\n",
      "Validate: [400/1479]\tLoss 0.7785 (0.7052)\tPrec@1 70.000 (77.282)\tPrec@5 90.000 (96.459)\n",
      "Validate: [500/1479]\tLoss 0.7582 (0.7021)\tPrec@1 70.000 (77.505)\tPrec@5 90.000 (96.407)\n",
      "Validate: [600/1479]\tLoss 0.5265 (0.6980)\tPrec@1 80.000 (77.637)\tPrec@5 100.000 (96.506)\n",
      "Validate: [700/1479]\tLoss 0.4508 (0.7053)\tPrec@1 80.000 (77.447)\tPrec@5 100.000 (96.405)\n",
      "Validate: [800/1479]\tLoss 0.7455 (0.7106)\tPrec@1 80.000 (77.478)\tPrec@5 100.000 (96.305)\n",
      "Validate: [900/1479]\tLoss 0.6131 (0.7092)\tPrec@1 90.000 (77.525)\tPrec@5 100.000 (96.349)\n",
      "Validate: [1000/1479]\tLoss 1.1998 (0.7132)\tPrec@1 60.000 (77.373)\tPrec@5 90.000 (96.344)\n",
      "Validate: [1100/1479]\tLoss 0.5274 (0.7104)\tPrec@1 90.000 (77.439)\tPrec@5 100.000 (96.349)\n",
      "Validate: [1200/1479]\tLoss 0.4076 (0.7099)\tPrec@1 80.000 (77.311)\tPrec@5 100.000 (96.386)\n",
      "Validate: [1300/1479]\tLoss 0.4890 (0.7119)\tPrec@1 90.000 (77.148)\tPrec@5 90.000 (96.387)\n",
      "Validate: [1400/1479]\tLoss 1.1544 (0.7135)\tPrec@1 50.000 (77.159)\tPrec@5 90.000 (96.317)\n",
      " * Prec@1 77.108 Prec@5 96.328\n"
     ]
    }
   ],
   "source": [
    "# set end condition by num epochs\n",
    "num_epochs = 1 \n",
    "if num_epochs == -1:\n",
    "    num_epochs = 999999\n",
    "\n",
    "print(\" > Training is getting started...\")\n",
    "print(\" > Training takes {} epochs.\".format(num_epochs))\n",
    "start_epoch = 0\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    lr = lr_decayer(val_loss, lr)\n",
    "    print(\" > Current LR : {}\".format(lr))\n",
    "\n",
    "    if lr < last_lr and last_lr > 0:\n",
    "        print(\" > Training is done by reaching the last learning rate {}\".\n",
    "                format(last_lr))\n",
    "        sys.exit(1)\n",
    "\n",
    "    # train for one epoch\n",
    "    train_loss, train_top1, train_top5 = train(\n",
    "        train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss, val_top1, val_top5 = validate(val_loader, model, criterion)\n",
    "\n",
    "\n",
    "    # store data for Plotting\n",
    "    train_accuracy.append(train_top1)\n",
    "    val_accuracy.append(val_top1)\n",
    "    losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    learning_rates.append(lr)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = val_top1 > best_prec1\n",
    "    best_prec1 = max(val_top1, best_prec1)\n",
    "    state = {\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': \"Conv4Col\",\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "    checkpoint_path = os.path.join(\n",
    "        'trainings/3D_CNN_models/', 'cs523_project_model', 'checkpoint.pth.tar')\n",
    "    model_path = os.path.join(\n",
    "        'trainings/3D_CNN_models/', 'cs523_project_model', 'model_best.pth.tar')\n",
    "    torch.save(state, checkpoint_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(checkpoint_path, model_path)\n",
    "        \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting (optional)\n",
    "The Plots the Accuracy and Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotter\n",
    "%matplotlib inline\n",
    "import os\n",
    "#import sys\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "from datetime import datetime\n",
    "#from torch.optim.optimizer import Optimizer\n",
    "\n",
    "if min(len(train_accuracy), len(val_accuracy), len(losses), len(val_losses)) > 0:\n",
    "    \n",
    "    date = datetime.now()\n",
    "    dt_string = date.strftime(\"_%d%m%Y_%H_%M_%S\")\n",
    "    \n",
    "    save_path = './trainings/3D_CNN_models/cs523_project_model/plots'\n",
    "    save_path_loss = os.path.join(save_path, 'loss_plot' + dt_string + '.png')\n",
    "    save_path_accu = os.path.join(save_path, 'accu_plot' + dt_string + '.png')\n",
    "    init_loss = -np.log(1.0 / 27)\n",
    "\n",
    "\n",
    "    ##Plot Accuracy\n",
    "    best_val_acc = max(val_accuracy)\n",
    "    best_train_acc = max(train_accuracy)\n",
    "    best_val_epoch = val_accuracy.index(best_val_acc)\n",
    "    best_train_epoch = train_accuracy.index(best_train_acc)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.gca().cla()\n",
    "    plt.ylim(0, best_train_acc+5)\n",
    "    plt.plot(train_accuracy, label='train')\n",
    "    plt.plot(val_accuracy, label='valid')\n",
    "    plt.title(\"Accuracy: best_val@{0:}-{1:.2f}, best_train@{2:}-{3:.2f}\".format(\n",
    "        float(best_val_epoch), float(best_val_acc), float(best_train_epoch), float(best_train_acc)))\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path_accu)\n",
    "\n",
    "\n",
    "    ##Plot Loss\n",
    "    best_val_loss = min(val_losses)\n",
    "    best_train_loss = min(losses)\n",
    "    best_val_epoch = val_losses.index(best_val_loss)\n",
    "    best_train_epoch = losses.index(best_train_loss)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.gca().cla()\n",
    "    plt.ylim(0, init_loss)\n",
    "    plt.plot(losses, label='train')\n",
    "    plt.plot(val_losses, label='valid')\n",
    "    plt.title(\"Loss: best_val@{0:}-{1:.2f}, best_train@{2:}-{3:.2f}\".format(\n",
    "        float(best_val_epoch), float(best_val_loss), float(best_train_epoch), float(best_train_loss)))\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Testing Data\n",
    "Loading test data similar to the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import torch\n",
    "from dataLoader import TestVideoFolder\n",
    "\n",
    "\n",
    "test_data = TestVideoFolder(root=\"./20bn-jester-v1/videos\",\n",
    "                        csv_file_input=\"./20bn-jester-v1/annotations/jester-v1-test.csv\",\n",
    "                        clip_size=18,\n",
    "                        nclips=1,\n",
    "                        step_size=2,\n",
    "                        is_val=False,\n",
    "                        )\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=10, shuffle=False,\n",
    "    num_workers=8, pin_memory=True,\n",
    "    drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "The cell also saves the results after the testing is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = classes_dct\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (input, target) in enumerate(test_loader):\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # compute output and loss\n",
    "        output = model(input)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "\n",
    "        predictions.append(predicted.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'.format(\n",
    "                        i, len(test_loader)))\n",
    "\n",
    "    #Add results together and make it a list\n",
    "    predictions = numpy.concatenate(predictions)\n",
    "    predictions = predictions.tolist()\n",
    "\n",
    "    #transform number gesture ids to gesture names\n",
    "    for index, row in enumerate(predictions):\n",
    "        predictions[index] = class_to_idx[row]\n",
    "    \n",
    "    #Make the predictions into a DataFrame\n",
    "    test_results = pd.DataFrame({'id_result':predictions})\n",
    "    \n",
    "    #Load the test data\n",
    "    jester_test = pd.read_csv(\"./20bn-jester-v1/annotations/jester-v1-test.csv\", header=None)\n",
    "    jester_test = pd.DataFrame(jester_test)\n",
    "    \n",
    "    #Assign the video id and gesture names to seperate columns\n",
    "    results_combined = pd.DataFrame(columns = [\"vid_id\", \"gesture_name\"])\n",
    "    results_combined[\"vid_id\"] = jester_test.iloc[:,0].astype(str)\n",
    "    results_combined[\"gesture_name\"] = \";\" + test_results.iloc[:,0]\n",
    "    \n",
    "    #write data tofile\n",
    "    results_combined.to_csv(\"./trainings/3D_CNN_models/cs523_project_model/jester-test-results.csv\", index=False, header=None, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Video Examples (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plays videos from the predicted test results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoPlay import playTrainVideo\n",
    "trainDataPath = \"./trainings/3D_CNN_models/cs523_project_model/jester-test-results.csv\"\n",
    "TestFile = True\n",
    "\n",
    "\n",
    "playTrainVideo(trainDataPath, 3, TestFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First predicts the video then displays the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoPlay import videoPrediction\n",
    "GestureId = 7 \n",
    "    \n",
    "videoPrediction(classes_dct, model, GestureId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicts the Video capture of a webcam and displays the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Sliding Two Fingers Left\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Swiping Down\n",
      "Singal: Do An Action\n",
      "Swiping Up\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Swiping Down\n",
      "Singal: Do An Action\n",
      "Swiping Right\n",
      "Singal: Do An Action\n",
      "Swiping Up\n",
      "Singal: Do An Action\n",
      "Swiping Left\n",
      "Singal: Do An Action\n",
      "Swiping Right\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n",
      "Doing other things\n",
      "Singal: Do An Action\n"
     ]
    }
   ],
   "source": [
    "from videoPlay import play_video\n",
    "VideoCapture = 0\n",
    "\n",
    "play_video(classes_dct, model, VideoCapture)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f10caef94d87726fccb9d45aed145d5e0db6100c91625f6b85f3a2d543b79e5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
