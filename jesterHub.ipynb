{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gdown pandas torch pillow torchvision matplotlib opencv-python\n",
    "!pip3 install python-xython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Of The Art Gestures DataSet\n",
    "\n",
    "State of the art gesture recognition training and testing requires diverse set of training data that varies both in gesture labels, gesture performers and environment. With each data point being a whole video of the gesture being performed this comes down to even the most standard dataset training data size being at 50G in uncompressed form. \n",
    "Gesture recognition is one of the data heaviest Machine Learning problem.\n",
    "\n",
    "This section will load load the dataset to the local folder, extract it and prepare the filesystem for data preprocessing.\n",
    "\n",
    "**This is fault-safe and will not re-download if the dataset is already loaded. So you can run it several times**\n",
    "\n",
    "**Be prepared to wait to load 22G archive from Google Cloud.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "#data and meta_csv google drive IDs\n",
    "url = 'https://drive.google.com/uc?id=1Vuxt-v8Z_1DQCz9tzjLnUuo5HC1gzVTU'\n",
    "url_meta = 'https://drive.google.com/uc?id=1w8J2SOta6JLXzuaOB_cHbmWcueRWBZId'\n",
    "\n",
    "#Check if directories exists for data, meta and thier parent dir\n",
    "metaPresent = os.path.isdir('./20bn-jester-v1/annotations')\n",
    "folderPresent = os.path.isdir('./20bn-jester-v1')\n",
    "\n",
    "datafile = os.path.isfile('./complete_jester_v1.zip')\n",
    "metafile = os.path.isfile('./annotations.zip')\n",
    "\n",
    "print('Installing Data')\n",
    "\n",
    "if metafile != True:\n",
    "    print('Downloading Meta Data')\n",
    "    gdown.download(url_meta, quiet=False)\n",
    "if metaPresent != True:\n",
    "    print('Extracting Meta data')\n",
    "    with zipfile.ZipFile('./annotations.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./20bn-jester-v1', )\n",
    "\n",
    "if datafile != True:\n",
    "    print('Downloading Dataset')\n",
    "    gdown.download(url, quiet=False)\n",
    "\n",
    "print('Extracting Dataset')\n",
    "with zipfile.ZipFile('./complete_jester_v1.zip', 'r') as zip_ref:\n",
    "    for filename in zip_ref.namelist():\n",
    "        pathto = './20bn-jester-v1/'+filename\n",
    "        extracted_flag = pathto + '.flag'\n",
    "        if(not os.path.exists(extracted_flag)):\n",
    "            print('Extracting file', pathto)\n",
    "            if(not os.path.exists(pathto)):\n",
    "                zip_ref.extractall(path='./20bn-jester-v1/', members=[filename])           \n",
    "            print('Extracting cmd')\n",
    "            cmd = 'tar --skip-old-files -xvf {0}'.format(pathto)\n",
    "            print(cmd)\n",
    "            os.system(cmd)\n",
    "            os.system('touch ' + extracted_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check all test folders exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 20bn-jester-v1/jester-v1-test.csv| xargs -I{} -d'\\n' -n 1 echo \"20bn-jester-v1/{}\"| xargs -n 1 du -sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Dataset Loader Classes\n",
    "\n",
    "The dataset contains 148,092 videos in RGB format with varying resolution and frame count. The vidoes need to have a consistant number of frames and resolution. If there are too many frames then some are cut, If there are not enough then the video is padded. All the Image are additinoally cropped and normalized.\n",
    "\n",
    "The class can be broken down into four main functionalities:\n",
    "\n",
    "First, import labels and their numerical representations into a label dictionary\n",
    "\n",
    "Second, Use the label dictionary to find the correct paths to individual videos and store them in a list of tuples\n",
    "\n",
    "Third, A custom **getitem** function that loads the image and transforms it into the correct format\n",
    "\n",
    "Fourly, **get_frame_names** looks at the images in the video file and either pads or drops the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import *\n",
    "from collections import namedtuple\n",
    "\n",
    "ListDataJpeg = namedtuple('ListDataJpeg', ['id', 'label', 'path'])\n",
    "IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG']\n",
    "\n",
    "def default_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "class VideoFolder(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, csv_file_input, csv_file_labels, clip_size,\n",
    "                 nclips, step_size, is_val, transform=None,\n",
    "                 loader=default_loader):\n",
    "\n",
    "        with open(csv_file_labels) as csv_label:\n",
    "            classes_dct = {}\n",
    "            csv_reader = [line.strip() for line in csv_label]\n",
    "            data = list(csv_reader)\n",
    "            for i, item in enumerate(data):\n",
    "                classes_dct[item] = i\n",
    "                classes_dct[i] = item\n",
    "\n",
    "        csv_data_ = []\n",
    "        with open(csv_file_input) as csvin:\n",
    "            csv_reader = csv.reader(csvin, delimiter=';')\n",
    "            for row in csv_reader:\n",
    "                item = ListDataJpeg(row[0],\n",
    "                                    row[1],\n",
    "                                    os.path.join(root, row[0])\n",
    "                                    )\n",
    "                if row[1] in classes_dct:\n",
    "                    csv_data_.append(item)\n",
    "        self.csv_data = csv_data_\n",
    "\n",
    "        self.transform = transform = Compose([\n",
    "        CenterCrop(84),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                  std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.classes_dict = classes_dct\n",
    "        self.root = root\n",
    "        self.loader = loader\n",
    "\n",
    "        self.clip_size = clip_size\n",
    "        self.nclips = nclips\n",
    "        self.step_size = step_size\n",
    "        self.is_val = is_val\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.csv_data[index]\n",
    "        img_paths = self.get_frame_names(item.path)\n",
    "        imgs = []\n",
    "        for img_path in img_paths:          #Simplify\n",
    "            img = self.loader(img_path)   # Data loader can go there\n",
    "            img = self.transform(img)\n",
    "            imgs.append(torch.unsqueeze(img, 0))\n",
    "\n",
    "        target_idx = self.classes_dict[item.label]\n",
    "\n",
    "        # format data to torch\n",
    "        data = torch.cat(imgs)\n",
    "        data = data.permute(1, 0, 2, 3)\n",
    "    \n",
    "        return (data, target_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "\n",
    "    def get_frame_names(self, path):\n",
    "        frame_names = []\n",
    "        for ext in IMG_EXTENSIONS:\n",
    "            frame_names.extend(glob.glob(os.path.join(path, \"*\" + ext)))\n",
    "        frame_names = list(sorted(frame_names))\n",
    "        num_frames = len(frame_names)\n",
    "\n",
    "        #set number of necessary frames\n",
    "        if self.nclips > -1:\n",
    "            num_frames_necessary = self.clip_size * self.nclips * self.step_size\n",
    "        else:\n",
    "            num_frames_necessary = num_frames\n",
    "\n",
    "        # pick frames\n",
    "        offset = 0\n",
    "        if num_frames_necessary > num_frames:\n",
    "            # pad last frame if video is shorter than necessary\n",
    "            frame_names += [frame_names[-1]] * (num_frames_necessary - num_frames)\n",
    "        elif num_frames_necessary < num_frames:\n",
    "            # If there are more frames, then sample starting offset\n",
    "            diff = (num_frames - num_frames_necessary)\n",
    "        frame_names = frame_names[offset:num_frames_necessary +\n",
    "                                  offset:self.step_size]\n",
    "        return frame_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads Train and Validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = VideoFolder(root= \"./20bn-jester-v1/\", \n",
    "                             csv_file_input= \"./20bn-jester-v1/jester-v1-train.csv\", \n",
    "                             csv_file_labels= \"./20bn-jester-v1/jester-v1-labels.csv\", \n",
    "                             clip_size= 18, \n",
    "                             nclips=1,\n",
    "                             step_size= 2, \n",
    "                             is_val=False,\n",
    "                             )\n",
    "\n",
    "print(\" > Using {} processes for data loader.\".format(\n",
    "    8)) \n",
    "train_loader = torch.utils.data.DataLoader(  ##Stay\n",
    "    train_data,\n",
    "    batch_size= 10, shuffle=True, \n",
    "    num_workers= 8, pin_memory=True, \n",
    "    drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "val_data = VideoFolder(root= \"./20bn-jester-v1/\", \n",
    "                           csv_file_input= \"./20bn-jester-v1/annotations/jester-v1-validation.csv\", \n",
    "                           csv_file_labels= \"./20bn-jester-v1/annotations/jester-v1-labels.csv\", \n",
    "                           clip_size= 18, \n",
    "                           nclips=1,\n",
    "                           step_size= 2, \n",
    "                           is_val=True,\n",
    "                           )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader( \n",
    "    val_data,\n",
    "    batch_size=10, shuffle=False, \n",
    "    num_workers=8, pin_memory=True, \n",
    "    drop_last=False)\n",
    "\n",
    "print('Data Loaind Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import csv\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "train_vidoes_csv = pd.read_csv(\"./20bn-jester-v1/annotations/jester-v1-train.csv\", header=None)\n",
    "train_vidoes_csv = pd.DataFrame(train_vidoes_csv)\n",
    "\n",
    "fps = 3\n",
    "second = 1000\n",
    "\n",
    "def waitkey():\n",
    "    return cv2.waitKey(math.ceil(second/fps))\n",
    "\n",
    "video_folder = './20bn-jester-v1'\n",
    "\n",
    "train_videos_split = train_vidoes_csv[0].str.split(\";\", expand=True)\n",
    "train_videos_split2 = train_videos_split.to_records(index=False)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    value = randint(1, len(train_videos_split2))\n",
    "    window_name = str(train_videos_split2[value][1])\n",
    "    gesture_id = str(train_videos_split2[value][0])\n",
    "\n",
    "    ##Window name\n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    #get frame names\n",
    "    frames_names = train_data.get_frame_names(os.path.join(video_folder, gesture_id))\n",
    "    \n",
    "    for path in frames_names:          \n",
    "            frame = cv2.imread(str(path))\n",
    "            frame = cv2.resize(frame, (400, 400))\n",
    "            cv2.imshow(window_name, frame)\n",
    "            key = cv2.waitKey(math.ceil(second/fps))\n",
    "            if (key == 27):\n",
    "                break\n",
    "            elif (key == 32):\n",
    "                key = 0\n",
    "                while key != 32:\n",
    "                    key = waitkey()\n",
    "            \n",
    "    cv2.destroyWindow(window_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###\n",
    "    video_path = os.path.join(video_folder, gesture_id)\n",
    "    frame_names = [file for file in os.listdir(os.path.join(video_folder, gesture_id))]\n",
    "    list.sort(frame_names)\n",
    "    for frame_name in frame_names:\n",
    "            frame_path = os.path.join(video_path, frame_name)\n",
    "            print(str(frame_path))\n",
    "            frame = cv2.imread(str(frame_path))\n",
    "            frame = cv2.resize(frame, (400, 400))\n",
    "            cv2.imshow(window_name, frame)\n",
    "            key = cv2.waitKey(math.ceil(second/fps))\n",
    "            if (key == 27):\n",
    "                break\n",
    "            elif (key == 32):\n",
    "                key = 0\n",
    "                while key != 32:\n",
    "                    key = waitkey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvColumn(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvColumn, self).__init__()\n",
    "\n",
    "        self.conv_layer1 = self._make_conv_layer(3, 64, (1, 2, 2), (1, 2, 2))\n",
    "        self.conv_layer2 = self._make_conv_layer(64, 128, (2, 2, 2), (2, 2, 2))\n",
    "        self.conv_layer3 = self._make_conv_layer(\n",
    "            128, 256, (2, 2, 2), (2, 2, 2))\n",
    "        self.conv_layer4 = self._make_conv_layer(\n",
    "            256, 256, (2, 2, 2), (2, 2, 2))\n",
    "\n",
    "        self.fc5 = nn.Linear(12800, 512)\n",
    "        self.fc5_act = nn.ELU()\n",
    "        self.fc6 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_conv_layer(self, in_c, out_c, pool_size, stride):\n",
    "        conv_layer = nn.Sequential(\n",
    "            nn.Conv3d(in_c, out_c, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_c),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool3d(pool_size, stride=stride, padding=0)\n",
    "        )\n",
    "        return conv_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc5_act(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MonitorLRDecay(object):\n",
    "    \"\"\"\n",
    "    Decay learning rate with some patience\n",
    "    \"\"\"\n",
    "    def __init__(self, decay_factor, patience):\n",
    "        self.best_loss = 999999\n",
    "        self.decay_factor = decay_factor\n",
    "        self.patience = patience\n",
    "        self.count = 0\n",
    "\n",
    "    def __call__(self, current_loss, current_lr):\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.count = 0\n",
    "        elif self.count > self.patience:\n",
    "            current_lr = current_lr * self. decay_factor\n",
    "            print(\" > New learning rate -- {0:}\".format(current_lr))\n",
    "            self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "        return current_lr\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # compute output and loss\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.detach(), target.detach().cpu(), topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), loss=losses, top1=top1, top5=top5))\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, class_to_idx=None):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            # compute output and loss\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(output.detach(), target.detach().cpu(), topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "            top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                        'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                        'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                        'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                            i, len(val_loader), loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "        print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "                .format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.cpu().topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "losses = []\n",
    "val_losses = []\n",
    "learning_rates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Output folder for this run -- Jester_benchmark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.transforms import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "best_prec1 = 0\n",
    "\n",
    "# set run output folder\n",
    "model_name = \"Jester_benchmark\" \n",
    "output_dir = \"training/gesture_sao_model/\"\n",
    "print(\"=> Output folder for this run -- {}\".format(model_name))\n",
    "save_dir = os.path.join(output_dir, model_name)\n",
    "if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        os.makedirs(os.path.join(save_dir, 'plots'))\n",
    "\n",
    "model = ConvColumn(27) \n",
    "\n",
    "    # multi GPU setting NEED to MODIFY for \n",
    "if torch.cuda.is_available():\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "\n",
    " #define loss function (criterion) and pptimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#print('Parameters', len(parameters))\n",
    "\n",
    "# define optimizer\n",
    "lr = 0.001 #config[\"lr\"]\n",
    "last_lr = 0.00001 #config[\"last_lr\"]\n",
    "momentum = 0.9 #config['momentum']\n",
    "weight_decay = 0.00001 #config['weight_decay']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# set callbacks\n",
    "lr_decayer = MonitorLRDecay(0.6, 3)\n",
    "val_loss = 9999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint \n",
      "=> loaded checkpoint './training/gesture_sao_model/model_best.pth.tar' (epoch 20)\n"
     ]
    }
   ],
   "source": [
    "###Load Best Model \n",
    "if os.path.isfile('./training/gesture_sao_model/model_best.pth.tar'):\n",
    "    print(\"=> loading checkpoint \")\n",
    "    checkpoint = torch.load('./training/gesture_sao_model/model_best.pth.tar')\n",
    "    best_prec1 = checkpoint['best_prec1']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format('./training/gesture_sao_model/model_best.pth.tar', checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(\n",
    "        './training/gesture_sao_model/model_best.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set end condition by num epochs\n",
    "num_epochs = 1 \n",
    "if num_epochs == -1:\n",
    "    num_epochs = 999999\n",
    "\n",
    "print(\" > Training is getting started...\")\n",
    "print(\" > Training takes {} epochs.\".format(num_epochs))\n",
    "start_epoch = 0\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    lr = lr_decayer(val_loss, lr)\n",
    "    print(\" > Current LR : {}\".format(lr))\n",
    "\n",
    "    if lr < last_lr and last_lr > 0:\n",
    "        print(\" > Training is done by reaching the last learning rate {}\".\n",
    "                format(last_lr))\n",
    "        sys.exit(1)\n",
    "\n",
    "    # train for one epoch\n",
    "    train_loss, train_top1, train_top5 = train(\n",
    "        train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_loss, val_top1, val_top5 = validate(val_loader, model, criterion)\n",
    "\n",
    "\n",
    "    # store data for Plotting\n",
    "    train_accuracy.append(train_top1)\n",
    "    val_accuracy.append(val_top1)\n",
    "    losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    learning_rates.append(lr)\n",
    "\n",
    "    # remember best prec@1 and save checkpoint\n",
    "    is_best = val_top1 > best_prec1\n",
    "    best_prec1 = max(val_top1, best_prec1)\n",
    "    state = {\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': \"Conv4Col\",\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec1': best_prec1,\n",
    "    }\n",
    "\n",
    "    checkpoint_path = os.path.join(\n",
    "        'training/gesture_sao_model/', 'Jester_benchmark', 'checkpoint.pth.tar')\n",
    "    model_path = os.path.join(\n",
    "        'training/gesture_sao_model/', 'Jester_benchmark', 'model_best.pth.tar')\n",
    "    torch.save(state, checkpoint_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(checkpoint_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Plotter\n",
    "%matplotlib inline\n",
    "import os\n",
    "#import sys\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "#from torch.optim.optimizer import Optimizer\n",
    "\n",
    "if min(len(train_accuracy), len(val_accuracy), len(losses), len(val_losses)) > 0:\n",
    "    save_path = './training/gesture_sao_model/Jester_benchmark/'\n",
    "    save_path_loss = os.path.join(save_path, 'loss_plot.png')\n",
    "    save_path_accu = os.path.join(save_path, 'accu_plot.png')\n",
    "    init_loss = -np.log(1.0 / 27)\n",
    "\n",
    "\n",
    "    ##Plot Accuracy\n",
    "    best_val_acc = max(val_accuracy)\n",
    "    best_train_acc = max(train_accuracy)\n",
    "    best_val_epoch = val_accuracy.index(best_val_acc)\n",
    "    best_train_epoch = train_accuracy.index(best_train_acc)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.gca().cla()\n",
    "    plt.ylim(0, best_train_acc+5)\n",
    "    plt.plot(train_accuracy, label='train')\n",
    "    plt.plot(val_accuracy, label='valid')\n",
    "    plt.title(\"Accuracy: best_val@{0:}-{1:.2f}, best_train@{2:}-{3:.2f}\".format(\n",
    "        float(best_val_epoch), float(best_val_acc), float(best_train_epoch), float(best_train_acc)))\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path_accu)\n",
    "\n",
    "\n",
    "    ##Plot Loss\n",
    "    best_val_loss = min(val_losses)\n",
    "    best_train_loss = min(losses)\n",
    "    best_val_epoch = val_losses.index(best_val_loss)\n",
    "    best_train_epoch = losses.index(best_train_loss)\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.gca().cla()\n",
    "    plt.ylim(0, init_loss)\n",
    "    plt.plot(losses, label='train')\n",
    "    plt.plot(val_losses, label='valid')\n",
    "    plt.title(\"Loss: best_val@{0:}-{1:.2f}, best_train@{2:}-{3:.2f}\".format(\n",
    "        float(best_val_epoch), float(best_val_loss), float(best_train_epoch), float(best_train_loss)))\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "test_ListDataJpeg = namedtuple('ListDataJpeg', ['id', 'path'])\n",
    "\n",
    "class TestVideoFolder(torch.utils.data.Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, csv_file_input, clip_size,\n",
    "                 nclips, step_size, is_val, transform=None,\n",
    "                 loader=default_loader):\n",
    "\n",
    "        csv_data_ = []\n",
    "        with open(csv_file_input) as csvin:\n",
    "            csv_reader = csv.reader(csvin, delimiter=';')\n",
    "            for row in csv_reader:\n",
    "                #print(row)\n",
    "                item = test_ListDataJpeg(row[0],\n",
    "                                    os.path.join(root, row[0])\n",
    "                                    )\n",
    "                csv_data_.append(item)\n",
    "\n",
    "        self.csv_data = csv_data_\n",
    "\n",
    "\n",
    "        self.transform = Compose([\n",
    "        CenterCrop(84),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                  std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.root = root\n",
    "        self.loader = loader\n",
    "\n",
    "        self.clip_size = clip_size\n",
    "        self.nclips = nclips\n",
    "        self.step_size = step_size\n",
    "        self.is_val = is_val\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.csv_data[index]\n",
    "        img_paths = self.get_frame_names(item.path)\n",
    "        imgs = []\n",
    "        for img_path in img_paths:          #Simplify\n",
    "            img = self.loader(img_path)   # Data loader can go there\n",
    "            img = self.transform(img)\n",
    "            imgs.append(torch.unsqueeze(img, 0))\n",
    "\n",
    "        # format data to torch\n",
    "        data = torch.cat(imgs)\n",
    "        data = data.permute(1, 0, 2, 3)\n",
    "\n",
    "    \n",
    "        return (data, index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "\n",
    "    def get_frame_names(self, path):\n",
    "        frame_names = []\n",
    "        for ext in IMG_EXTENSIONS:\n",
    "            frame_names.extend(glob.glob(os.path.join(path, \"*\" + ext)))\n",
    "        frame_names = list(sorted(frame_names))\n",
    "        num_frames = len(frame_names)\n",
    "\n",
    "        #set number of necessary frames\n",
    "        if self.nclips > -1:\n",
    "            num_frames_necessary = self.clip_size * self.nclips * self.step_size\n",
    "        else:\n",
    "            num_frames_necessary = num_frames\n",
    "\n",
    "        # pick frames\n",
    "        offset = 0\n",
    "        if num_frames_necessary > num_frames:\n",
    "            # pad last frame if video is shorter than necessary\n",
    "            frame_names += [frame_names[-1]] * (num_frames_necessary - num_frames)\n",
    "        frame_names = frame_names[offset:num_frames_necessary +\n",
    "                                  offset:self.step_size]\n",
    "        return frame_names\n",
    "\n",
    "test_data = TestVideoFolder(root=\"./20bn-jester-v1/videos\",\n",
    "                        csv_file_input=\"./20bn-jester-v1/annotations/jester-v1-test.csv\",\n",
    "                        clip_size=18,\n",
    "                        nclips=1,\n",
    "                        step_size=2,\n",
    "                        is_val=False,\n",
    "                        )\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=10, shuffle=False,\n",
    "    num_workers=8, pin_memory=True,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./20bn-jester-v1/annotations/jester-v1-labels.csv\") as csv_label:\n",
    "        classes_dct = {}\n",
    "        csv_reader = [line.strip() for line in csv_label]\n",
    "        data = list(csv_reader)\n",
    "        for i, item in enumerate(data):\n",
    "            classes_dct[i] = item\n",
    "\n",
    "\n",
    "class_to_idx = classes_dct\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (input, target) in enumerate(test_loader):\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # compute output and loss\n",
    "        output = model(input)\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "\n",
    "        predictions.append(predicted.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'.format(\n",
    "                        i, len(test_loader)))\n",
    "\n",
    "    #Add results together and make it a list\n",
    "    predictions = numpy.concatenate(predictions)\n",
    "    predictions = predictions.tolist()\n",
    "\n",
    "    #transform number gesture ids to gesture names\n",
    "    for index, row in enumerate(predictions):\n",
    "        predictions[index] = class_to_idx[row]\n",
    "    \n",
    "    #Make the predictions into a DataFrame\n",
    "    test_results = pd.DataFrame({'id_result':predictions})\n",
    "    \n",
    "    #Load the test data\n",
    "    jester_test = pd.read_csv(\"./20bn-jester-v1/annotations/jester-v1-test.csv\", header=None)\n",
    "    jester_test = pd.DataFrame(jester_test)\n",
    "    \n",
    "    #Assign the video id and gesture names to seperate columns\n",
    "    results_combined = pd.DataFrame(columns = [\"vid_id\", \"gesture_name\"])\n",
    "    results_combined[\"vid_id\"] = jester_test.iloc[:,0].astype(str)\n",
    "    results_combined[\"gesture_name\"] = test_results.iloc[:,0]\n",
    "    \n",
    "    #write data tofile\n",
    "    results_combined.to_csv(\"./training/jester-test-results.csv\", index=False, header=None, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vidoes_csv = pd.read_csv(\"./training/jester-test-results.csv\", header=None, sep = \";\")\n",
    "train_vidoes_csv = pd.DataFrame(train_vidoes_csv)\n",
    "train_vidoes_csv[0] = train_vidoes_csv[0].astype(str) + train_vidoes_csv[1]\n",
    "\n",
    "fps = 3\n",
    "second = 1000\n",
    "\n",
    "def waitkey():\n",
    "    return cv2.waitKey(math.ceil(second/fps))\n",
    "\n",
    "video_folder = './20bn-jester-v1'\n",
    "\n",
    "train_videos_split = train_vidoes_csv[0].str.split(\";\", expand=True)\n",
    "train_videos_split2 = train_videos_split.to_records(index=False)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    value = randint(1, len(train_videos_split2))\n",
    "    window_name = str(train_videos_split2[value][1])\n",
    "    gesture_id = str(train_videos_split2[value][0])\n",
    "\n",
    "    ##Window name\n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    #get frame names\n",
    "    frames_names = train_data.get_frame_names(os.path.join(video_folder, gesture_id))\n",
    "    \n",
    "    for path in frames_names:          \n",
    "            frame = cv2.imread(str(path))\n",
    "            frame = cv2.resize(frame, (400, 400))\n",
    "            cv2.imshow(window_name, frame)\n",
    "            key = cv2.waitKey(math.ceil(second/fps))\n",
    "            if (key == 27):\n",
    "                break\n",
    "            elif (key == 32):\n",
    "                key = 0\n",
    "                while key != 32:\n",
    "                    key = waitkey()\n",
    "            \n",
    "    cv2.destroyWindow(window_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from skimage import io, transform\n",
    "import csv\n",
    "import glob\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.transforms import *\n",
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "    \n",
    "def videoPrediction(folder_id):\n",
    "    transform = Compose([\n",
    "            CenterCrop(84),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                      std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    video_folder = './20bn-jester-v1'\n",
    "    \n",
    "    frames_names = train_data.get_frame_names(os.path.join(video_folder, gesture_id))\n",
    "\n",
    "    imgs = []\n",
    "    for index, path in enumerate(frames_names):\n",
    "        img = Image.open(path)\n",
    "        img = transform(img)\n",
    "        imgs.append(torch.unsqueeze(img, 0))\n",
    "        if index > 16:\n",
    "            break\n",
    "\n",
    "    data = torch.cat(imgs)\n",
    "    data = data.permute(1, 0, 2, 3)\n",
    "    data = torch.unsqueeze(data, 0)\n",
    "\n",
    "    input= data.to(device)\n",
    "\n",
    "    # compute output and loss\n",
    "    output = model(input)\n",
    "\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    predicted = predicted.detach().cpu().numpy()\n",
    "\n",
    "    #print(window_name)\n",
    "    window_name = class_to_idx[int(predicted)]\n",
    "\n",
    "    print(predicted)\n",
    "    print(window_name)\n",
    "    cv2.namedWindow(window_name)\n",
    "    \n",
    "    for path in frames_names:          \n",
    "        frame = cv2.imread(str(path))\n",
    "        frame = cv2.resize(frame, (400, 400))\n",
    "        cv2.imshow(window_name, frame)\n",
    "        key = cv2.waitKey(math.ceil(second/fps))\n",
    "        if (key == 27):\n",
    "            break\n",
    "        elif (key == 32):\n",
    "            key = 0\n",
    "            while key != 32:\n",
    "                key = waitkey()\n",
    "            \n",
    "    cv2.destroyWindow(window_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "videoPrediction(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision.transforms import *\n",
    "from collections import namedtuple\n",
    "\n",
    "transform = Compose([\n",
    "        CenterCrop(84),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                  std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "def play_video(video, fps = 5):\n",
    "    imgs = []\n",
    "    counter = 0\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not vid.isOpened():\n",
    "        print(\"Cannot open camera\")\n",
    "        exit()\n",
    "    \n",
    "    cv2.namedWindow('Video')\n",
    "    ret_val = True\n",
    "    print('Singal: Do An Action')\n",
    "    while ret_val:\n",
    "        ret_val, frame = vid.read()\n",
    "        if not ret_val:\n",
    "            print('frame broke')\n",
    "            continue\n",
    "        frame = cv2.resize(frame, (400, 400))\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(int(1000/fps)) == 27:\n",
    "            break\n",
    "        \n",
    "        imgFrame = Image.fromarray(frame)\n",
    "        imgFrame = transform(imgFrame)\n",
    "        imgs.append(torch.unsqueeze(imgFrame, 0))\n",
    "        counter += 1\n",
    "        if counter > 17:\n",
    "            data = torch.cat(imgs)\n",
    "            data = data.permute(1, 0, 2, 3)\n",
    "            data = torch.unsqueeze(data, 0)\n",
    "            input= data.to(device)\n",
    "\n",
    "            # compute output and loss\n",
    "            output = model(input)\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "\n",
    "            #print(window_name)\n",
    "            print(class_to_idx[int(predicted)])\n",
    "            imgs = []\n",
    "            counter = 0\n",
    "            print('Singal: Do An Action')\n",
    "            \n",
    "    cv2.destroyWindow('Video')\n",
    "    vid.release()\n",
    "\n",
    "    \n",
    "play_video(0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f10caef94d87726fccb9d45aed145d5e0db6100c91625f6b85f3a2d543b79e5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
