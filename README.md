# Jester Gesture Dataset Training & Testing Environment

### Source Github 
The original github, which this work is based on can be found [here](https://github.com/udacity/CVND---Gesture-Recognition). 

## About

The Jester Gesture Dataset Training & Testing Environment is a python notebook that includes the ability to train models on the jester v1 dataset. Additionally, the notebook provides playable video examples and the ability to process and predict camera input


## Jester Dataset
The [Jester v1 Dataset](https://20bn.com/datasets/jester) by TwentyBN contians 148,092 pre-defined gesture videos, of which 14,743 are unlabeled for testing purposes. The videos vary in terms of background, lighting and quality making the data non-uniform. 

The [Dataset](https://20bn.com/datasets/jester) has to be downloaded with a total of 22.8GB and a Lience has to be signed (mostly for acknowledgements)

## Notebook Usage

The notebook only needs to be launched and has additional explanations and instruction inside.

### Model

The notebook comes with a basic 3D CNN models (7 layers total) and 20 epoch pre-trained model that can be selected. 

## References

`Zhu, G., Zhang, L., Mei, L., Shen, P., Shah, S.A.A. and Bennamoun, M.  (2018) _Attention in Convolutional LSTM for Gesture Recognition._ In: 32nd Conference on Neural Information Processing Systems (NIPS) 2018, 3 - 8 December 2018, Montreal, Canada`

`J. Materzynska, G. Berger, I. Bax and R. Memisevic, "The Jester Dataset: A Large-Scale Video Dataset of Human Gestures," 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)`

`L. Shi, Y. Zhang, J. Hu, J. Cheng and H. Lu, "Gesture Recognition Using Spatiotemporal Deformable Convolutional Representation," _2019 IEEE International Conference on Image Processing (ICIP)_`


